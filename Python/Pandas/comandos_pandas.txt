pip install pandas 
pip install jupyterlab

cmd -> jupyter notebook --notebook-dir=[ctrl+v caminho da pasta]
- ou apenas 'jupyter notebook' se ja estiver na pasta certa

No notebook: New -> Python 3.

opcional: pip install xlwt openpyxl #Excel xlrd(Consultar documentação)
pip install SQLAlchemy 		        #Permite trabalhar com databases
pip install psycopg2-binary 	    #Permite trabalhar com postgresql

### Comandos básicos ###
!!! Arquivos exemplo usados: Survey 2019 CSV
!!! https://insights.stackoverflow.com/survey
=import pandas as pd
=df = pd.read_csv('data/survey_results_public.csv') #Ler arquivos CSV diretamente da pasta apontada cd
=df #Após receber o arquivo, apenas isso vai imprimir a informação contida
=df.shape      #Mostra a quantidade de linhas e colunas
=df.info()     #Mesma coisa do shape, com mais detalhe
=df.head(5)    #Mostra apenas os 5 primeiros resultados
=df.head(10)   #Mostra apenas os 10 últimos resultados
=pd.set_option('display.max_columns', 85)   #Vai mostrar no máximo 85 colunas
=pd.set_option('display.max_rows', 85)      #Mostra Máximo 85 Linhas
=schema_df = pd.read_csv('data/survey_results_schema.csv')
schema_df   #Vai abrir o outro arquivo. Com tudo acima aberto, vai ter como ler os 2 ao mesmo tempo

people = {
"first" = ["Corey", "Jane", "John"]
"last" = ["Schafer", "Doe", "Doe"]
"email" = ["CoreyMSchafer@gmail.com", "JaneDoe@gmail.com", "JohnDoe@gmail.com"]
}

### import pandas as pd ###

df = pd.DataFrame(People)
# Isso vai importar a informação desse dicionário e converter em uma Planílha com Linhas e Colúnas 
df['email'] 	
# Mostra apenas a linha emails. etc. RECOMENDADO
OU df.email 	
# MUITO CUIDADO COM OS MÉTODOS INTERNOS DO PANDA
df.columns 	
# Mostra o nome das colunas
df[['first', 'email']] 
# Para ler múltiplas colúnas

### iloc Permite acessar linhas por localização de inteiros ###

df.iloc[0] 
# Vai retornar Tudo que estiver na coluna 0. Corey Schaffer Email] ... df.iloc[[0, 1]] para múltiplas colunas
df.iloc[[0, 1], 2] 
# Vai mostrar o Email dos valores 0 e 1
df.loc[[0, 1], email]
df.loc[2, 'last'] = 'Smith' 
# Acessa a Coluna 2, linha 'Last'. O que tiver ali, va i mudar para 'Smith'
df.at[2, 'last'] = 'Doe' 
# Faz a mesma coisa. Acessar a documentação para mais detalhes

### Filtros / Filtrando ###

# filt = (df['email'] == 'JohnDoe@email.com') 
# Vai filtrar o arquivo e procurar por esse valor em específico
df[filt] 
# Vai printar o valor filtrado
df[filt]['last'] 
# Vai printar o valor filtrado, mas apenas a linha 'last' em específico
df[filt]['last'] = 'Smith' 
# Vai Tentar mudar o valor, mas vai dar um erro complicado. Para passar por cima, alterar o código da seguinte maneira:
df.loc[filt, 'last'] = 'Smith' 
# Para finalmente alterar um valor após filtrar
df['email'].str.lower() 
# Vai retornar valores em 'email' em minúsculo
df['email'] = df['email'].str.lower() 
# Vai alterar esses valores para minúsculo

### apply, map, applymap, replace: ###

df['email'].apply(len) 
# Vai retornar o tamanho de cada índice em 'email'
df.applymap(len) 
# Vai retornar o tamanho de cada índice/elemento em Tudo no arquivo
df.applymap(str.lower) 
# Mesma coisa acima, retorna tudo minúsuclo. Vai dar erro se tiver número. Métodos MAP funcionam apenas em 'Series'.
df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'}) 
# Dentro de 'first', procura e altera/muda 'Corey' e 'Jane' para 'Chris' e 'Mary'
df['first'].replace({'Corey': 'Chris', 'Jane': 'Mary'}) 
# Mesma coisa acima, mas pode evitar erros de 'NaN'(Not a Number).
df['first'] = df['first'].replace({'Corey': 'Chris', 'Jane': 'Mary'}) 
# Faz a alteração
df['Hobbyist'] -> df['Hobbyist'].map({'Yes': True, 'No': False}) 
# Seleciona Hobbyist e converte tudo que está Yes;No para True;False
df['Hobbyist'] = df['Hobbyist'] -> df['Hobbyist'].map({'Yes': True, 'No': False}) 
# Faz a alteração. Se tiver algo diferente de Yes;No, se torna 'NaN'. Nesse caso, inves de .map, usar .replace.

#def update_email(email):
	return email.upper() 
df['email'].apply(update_email) #Retorna tudo de 'email' em maíusculo
df['email'] = df['email'].apply(update_email) #Implementa a mudança
###
df.columns      
# Dentro vamos encontrar 'Hobbyist'
df['Hobbyist']  
# Vai mostrar o conteúdo/descrição
df['Hobbyist'].value_counts() 
# Vai mostrar quantas pessoas responderam "Yes" ou "No" no documento
df.loc[0] 	
# Vai mostrar todas as respostas dessa pessoa em específico
df.loc[0:5, 'Hobbyist'] 
# Mostra as respostas das pessoas entre 0 a 5. Se fizer com Slice ':', não pode estar entre colchetes []
df.loc[ [email pessoal de alguemn] ] 
# Vai mostrar todas as informações da pessoa que possui esse email. ETC

### INDEXANDO ### 
Em uma planilha 3x3 com Nome, Sobrenome e Email, dentro de um elemento chamado planilha:

df.set_index('email') 
# Vai deixar  email em relevância, em primeiro
df.set_index('email' inplace = True)
df.reset_index(inplace = True) 
# Reseta
pd.read_csv('data/survey_results_public.csv', index_col ='Respondent')
# Esse index no final vai mostrar apenas "Respondent" como index
df.loc[1] 
# Em seguida isso mostraria apenas o primeiro em "Respondent"
pd.read_csv('data/survey_results_public.csv', index_col ='Column')
# Esse index no final vai mostrar apenas "Column" como index 
E em seguida schema_df.loc['Hobbyist'] 
# Vai mostrar certinho a pergunta e resposta sobre Hobbyist
schema_df.loc['MgrIdiot', 'QuestionText'] 
# Esse texto vai estar Truncado, para destruncar, adicionar ', 'QuestionText'' no final
schema_df.sort_index() 
# Vai organizar em ordem alfabetica, A até Z
schema_df.sort_index(ascending = False) 
# De Z até A

### Alterando/Mudando o nome da linha/coluna: ###
["first", second", "email"], quero mudar para ["first_name", "second_name", "email"]

df.columns = ["first_name", "second_name", "email"]
df.columns = [x.upper() for x in df.columns] Deixa tudo maiusculo
df.columns = [df.columns.str.replace(' '), ('_')] Muda os espaços para _, ou o que quiser.
df.rename(columns={'first_name': 'first', 'last_name': 'last'}, inplace=True) 
# Isso RENOMEIA as colunas first_name para first e last_name para last. #
# inplace true para Alterar dentro do arquivo, não apenas no Jupyter    #

### Adicionando e Removendo Rows e Columns / Linhas e Colunas: ###

df['first'] + ' ' + ['last'] 
# Isso vai unir first com last entre um espaço ' '.
df['full_name'] = df['first'] + ' ' + ['last'] 
# Vai aplicar a mudança
df.drop(columns=['first', 'last'], inplace=True)
# Vai remover as colunas 'first' e 'last'. inplace true vai aplicar a mudança
df['full_name'].str.split(' ', expand=true) 
# Vai retornar os valores em uma Lista [valor1 valor 2 etc]. expand true vai retornar organizado em uma lista
df[['first', 'last']] = df['full_name'].str.split(' ', expand=true) 
# Vai aplicar as mudanças
df.append({'first': 'Tony'}, ignore_index=True) 
# Vai adicionar apenas 'Tony' na Series. Como não tem mais nada, as outras informações vão ser NaN.
df.append(df2, ignore_index=True, sort=false) 
# Vai fundir;Adicionar um dataframe com outro; DF1 com DF2. Sort False para não organizar a lista
df.drop(index=4) 
# Vai remover;deletar o index 4 inteiro. inplace true para aplicar a mudança
df.drop(index=[df['last'] == 'Doe'].index) 
# Vai procurar e remover;deletar todos os index em 'last' com valor 'Doe'.
filt = df['last'] == 'Doe' -> df.drop(index=df[filt].index)  
#Exatamente a mesma coisa acima, apenas um código mais limpo.

### Sorting / Organizando: ###
df.sort_values(by='last', ascending=False) 
# Retorna um dataframe organizado em ordem alfabetica A-Z, pela series 'last'. Ascending False = Z-A. inplace=True para aplicar as mudanças
df.sort_values(by=['last', 'first'], ascending='False', 'True') 
# Vai aplicar apenas para 'first'
df.sort_index() 
# Vai reorganizar tudo como estava antes
df.['last'].sort_values()
df.sort_values(by='Country', inplace = True) -> df['Country'].head(50) 
# Vai organizar e imprimir 50 primeiros resultados de 'Country' em ordem A-Z. Se for acessar multiplas listas, usar Colchetes df.sort_values(by=['Country', 'Salary'])
df['Salary'].nlargest(10) 
# Vai retornar  os 10 maiores valores em 'Salary'
df.nlargest(10, 'Salary') 
# Vai retornar os 10 maiores valores em 'Salary', mas com mais detalhes.
df.nsmallest[''] 
# Vai fazer a mesma coisa, mas retornar os Menores.

### Limpando Data / Cleaning data and missing values: ###
df.dropna(axis = 'index', how='any', subset=['email','etc']) 
# Vai limpar todos os NaN, 'Axis Index' ou 'How Column'. 'How Any' mostra qualquer valor NaN e 'How All' drop row when all values in that row are missing. 'Subset Email' retorna rows que tem emails preenchidos. 'inplace True' para alterar.
df.replace('NA', np.nan, inplace=True) 
# Importando Numpy, isso vai substituir;replace os valores NA;NaN em np.nan 
df.isna() 
# Retorna uma lista de rows/columns;linhas/colunas que tem 'NA' ou 'NaN'
df.fillna('MISSING') 
# Retorna uma lista onde 'NA' ou 'NaN' são substuidos por 'MISSING'. inplace=True para Alterar
df.dtypes #Retorna uma lista informando o tipo (Objeto, etc)
df['age'].mean() 
# Vai dar erro por ser Float
df['age'] = df['age'] .astype(int) # Vai tentar converter para Int, se tiver valores faltando, vai dar erro. ###Usando astype(float) vai consertar
df['age'].mean() 
# Vai retornar a média do valor.
na_vals = ['NA', 'MISSING']
df = pd.read_csv('data/survey_results_public.csv', na_values=na_vals) 
# Isso vai substituir direto os valores 'NA';'NaN' para 'MISSING'
df['Year'].unique() 
# Retorna os valores únicos

### Data e Hora / Date and Time Series Data: ###
df.head() -> df.loc[0, 'Date'].day_ name -> df['Date'] = pd.to_datetime(df['Date'])-> df.loc[0, 'Date']0.day_name() 
#Vai retornar o valor 'Friday', um dia da semana.
d_parser = lambda x: pd.datetime.strptime(x, '%Y-%m=%d %I-%p')
df = pd.read_csv('arquivo.csv', parse_dates=['Date']), date_parser=d_parser) 
# Vai converter o dia e hora para um formato melhor. convert date and time
df['Date'].dt.day_name() -> df['DayOfWeek'] = # df['Date'].dt.day_name() -> df['Date'].min();max() ->  
# dt.dayname mostra o dia da semana. O codigo seguinte mostra a coluna DayOfWeek junto com DayName. O codigo seguinte retorna o primeiro e ultimo resultado, se usar max - min, vai retornar quantos Dias totais existem. 
filt = (df['Date'] >= 2020) -> df.loc[filt] 
# Filtra datas acima de 2020 e depois retorna
filt = (df['Date'] >= 2019) & filt = (df['Date'] < 2020)
# Exclusivamente para 2019 e menor que 2020
filt = (df['Date'] >= pd.to_datetime(2019-01-01)) & filt = (df['Date'] < pd.to_datetime(2020-01-01)) 
# Mais especifico
df.set_index('Date', inplace=True) -> df['2019'] -> df['2020-01':'2020-02']['Close'].mean() ->  
# Retorna date como index da lista. #Retorna apenas os de 2019. #Depois usa Slicing;Fatiamento para fazer uma range entre 2020 01 e 2020 02, com 'Close' acessa apenas essa lista, e .mean mostra a média.
df['2020-01-01']['High'].max() -> df['2020-01-01']resample('D').max() -> 
# Retorna o valor máximo na coluna high #O resample redimensiona e formata dia;data;hora;etc; Consultar Documentação Pandas Pydata!! Vai retornar todas as series com os Maiores valores por DATA('D') ...
... highs = df['2020-01-01']resample('D').max() -> highs['2020-01-01'] 
# E enfim, mostra o valor nessa data em específico
%matplotlib inline -> highs.plot() 
# Tem que instalar o matplot primeiro, daí ele vai retornar um Chart 
df.resample('W').mean() -> df.resample('W').agg({'Close: 'mean', 'High': 'max', 'Low': 'min', 'Volume': 'sum'})  
# Resample por Semana, mean mostra a média semanal. #Retorna uma lista onde Close Mean(Média) High Max(Máximo) Low Min(Mínimo) Volume Sum (Sumário)

### Reading/Wrinting Data to different Sources ; Lendo/Escrevando Data para fontes diferentes: ###
pip install xlwt openpyxl xlrd #Excel (Consultar documentação)
pip install SQLAlchemy #Permite trabalhar com databases (Consultar documentação)
pip install psycopg2-binary #Permite trabalhar com postgresql (Consultar documentação)

### ETC ###
import pandas as pd -> 
df = pd.read.csv('data/arquivo.csv', index_col='Respondent') 
# Abre o arquivo e armazena em df, em Respondent
schema_df = pd.read_csv('data/arquivo.csv', index_col='Column') 
# Armazena em schemadf, em Respondent
pd.set_option('display.max_columns', 85) -> pd.set_option('display.max_rows', 85) 
# Máximo 85 Linhas ; Colunas
filt = (df['Country'] == 'India') -> india_df = df.loc[filt] -> india_df.head() 
# Filtra India #Passa o filtro para indiadf #Head vê s primeiras informações
india_df.to_csv('data/modificado.csv') 
# Vai exportar o filtro acima para um novo arquivo csv 
india_df.to_csv('data/modificado.tsv', sep='\t') 
# Inves de separar por virgulas, eh separado por Tabs \t; para melhor leitura. #
india_df.to_excel('data/modified.xlsx') 
# Converte para xlsx, que é a extensão direta do Excel
test = pd.read_excel('data/modified.xlsx', index_column='Respondent') 
# Executa o arquivo direto pelo Excel e armazena em test
india_df.to_json('data/modified.json', orient='records', ines=True) 
# Vai converter para json em forma de dicionário 
# Records coverte para algo parecido com lista #Lines True = mostra enumerado as informações com 1 2 3 4 ...
test = pd.read_json(india_df.to_json('data/modified.json', orient='records', ines=True) 
# Lê o .json e armazena em 'test', quando executar vai estar totalmente organizado igual o padrão Jupyter Panda
from sqlalchemy import create_engine -> import psycopg2 -> engine = create_engine('postrgesql://dbuser:dbpass@localhost:4532/pasta_db') 
# Usando um DB com Senha e Login, permite trabalhar com arquivos em DB, dbuser:dbpass são login e senha (ver dps como esconder isso); o resto é a URL do Host e Pasta do DB 
india_df.to_sql('sample_table', engine, if_exists='replace') 
# IndiaDF eh o DF que queremos exportar, Para SQL, Vai criar uma pasta sample table, e chamamos a engine (O código que criamos acima). Assim vai acessar e fazer alteração diretamente no DB. Se existir, substituir(Ou negar, dependendo da situação)
sql_df = pd.read_sql('SELECT * FROM sample_table', engine, index_column='Respondent') 
# Importa os atributos de SampleTable, abre e lê em SQL no index 'Respondent'. útil para DBs grandes
post_df = pd.read_json('[URL.. SEM OS COLCHETES!!]') 
#Lê data diretamente por URL

### ERRO: ###
###'Can only concatenate str to str': df['Year'] = df['Year'].astype(float). 
Se não der certo: df['Year'].unique() #Retorna os valores únicos, procurar valores que não são Inteiros e converter para Int: df['Year'].replace('string123', 0, inplace=True). # Assim, o primeiro codigo funciona 
###str object has no attribute 'nome_da_string': df['Date'] = pd.to_datetime(df['Date']). Pode ser que dê 'Unknow string format 2000-00-00 00-PM': df['Date'] = pd.to_datetime(df['Date']), format='%Y-%m=%d %I-%p') [Consultar documentação]

REVER AULA 9 https://www.youtube.com/watch?v=KdmPHEnPJPs&list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS&index=9